# 调试记录
## 断电后节点异常

一般都是etcd问题
使用kubectl 命令提示6443，查看api-server的容器日志，发现2379也就是ectd连接失败

查看etcd容器日志，发现是






### 多节点etcd
```bash
ssh -i ./tkg.id_rsa capv@10.197.1.236
sudo su root 
tar -cvf etcd_db.tar /var/lib/etcd/member/snap/db
chown capv:users ./etcd_db.tar
exit
exit

sudo scp  -i ./tkg.id_rsa capv@10.197.1.236:/home/capv/etcd_db.tar  ./etcd_db.tar
tar -xvf ./etcd_db.tar
mv ./var/lib/etcd/member/snap/db ./etcd_db
rm -rf ./var
bolt compact  -o ./etcd_db2  ./etcd_db
bolt check ./etcd_db2
scp -i ./tkg.id_rsa ./etcd_db2  capv@10.197.1.236:/home/capv/etcd_fixed.db

ssh -i ./tkg.id_rsa capv@10.197.1.236
sudo su root
cp /var/lib/etcd/member/wal /var/lib/etcd/member/wal2
rm -rf /var/lib/etcd/member/wal
cp /var/lib/etcd/member/snap /var/lib/etcd/member/snap2
rm -rf /var/lib/etcd/member/snap/*
cp /home/capv/etcd_fixed.db  /var/lib/etcd/member/snap/db
reboot
```


### 备份
```yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: backup
  namespace: kube-system
spec:
  concurrencyPolicy: Allow
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - args:
            - -c
            - etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt
              --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key
              snapshot save /backup/etcd-snapshot-$(date +%Y-%m-%d_%H:%M:%S_%Z).db
            command:
            - /bin/sh
            env:
            - name: ETCDCTL_API
              value: "3"
            image: k8s.gcr.io/etcd:3.4.3-0
            imagePullPolicy: IfNotPresent
            name: backup
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /etc/kubernetes/pki/etcd
              name: etcd-certs
              readOnly: true
            - mountPath: /backup
              name: backup
          - args:
            - -c
            - find /backup -type f -mtime +30 -exec rm -f {} \;
            command:
            - /bin/sh
            env:
            - name: ETCDCTL_API
              value: "3"
            image: k8s.gcr.io/etcd:3.4.3-0
            imagePullPolicy: IfNotPresent
            name: cleanup
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
            volumeMounts:
            - mountPath: /backup
              name: backup
          dnsPolicy: ClusterFirst
          hostNetwork: true
          nodeName: YOUR_MASTER_NODE_NAME
          restartPolicy: OnFailure
          schedulerName: default-scheduler
          securityContext: {}
          terminationGracePeriodSeconds: 30
          volumes:
          - hostPath:
              path: /etc/kubernetes/pki/etcd
              type: DirectoryOrCreate
            name: etcd-certs
          - hostPath:
              path: /opt/etcd_backups
              type: DirectoryOrCreate
            name: backup
  schedule: 0 */6 * * *
  successfulJobsHistoryLimit: 3
  suspend: false

```

### 参考链接
[为 Kubernetes 运行 etcd 集群](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/)
[#restoring-a-cluster](https://etcd.io/docs/v3.6/op-guide/recovery/#restoring-a-cluster)

### 单节点
如果没有恢复成功，应该只能将ectd数据清除，重新起集群




### 重置

```
kubeadm reset
rm -rf /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf /etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki /etc/cni/net.d
ipvsadm --clear
rm -rf $HOME/.kube/

重置网卡
ifconfig cni0 down && ip link delete cni0
ifconfig flannel.1 down && ip link delete flannel.1
rm -rf /var/lib/cni/

systemctl daemon-reload
systemctl restart containerd
```

#### 重置后相关问题
- 节点not ready：重启容器运行时，kubelet
- 节点只能访问自己机器上的pod：网络问题，多半是没有路由，重置时未清理网卡
- 






